# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zLjMS-V-RlF6qwurATiEcxLk21zZp1gg
"""

!pip install datasets # 데이터셋 패키지 다운로드

from datasets import load_dataset # Huggingface 데이터셋 패키지 import
data = load_dataset("sepidmnorozy/Korean_sentiment") # 데이터 다운로드

# Commented out IPython magic to ensure Python compatibility.
# connect google drive
from google.colab import drive
drive.mount('/content/drive')
# Download konlpy
# %cd ./drive/MyDrive/Colab\ Notebooks/
! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd ./Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab_light_220429.sh

#from konlpy.tag import Mecab
#mecab = Mecab()

#sentence = "실용자연어처리 실습진행중입니다."
#print(mecab.morphs(sentence))
#print(mecab.nouns(sentence))
#print(mecab.pos(sentence))

print("데이터 타입 : ",type(data))
print("데이터 구조 : ", data)
print("데이터 키 :",data.keys())
print(data['train'][0]) # 실제 데이터 확인

# Korean_sentiment 데이터셋은 학습/검증/테스트 셋이 분리되어있음
train_data = data['train'] # 변수 만들어 데이터 불러와 저장
dev_data = data['validation']
test_data = data['test']

print(train_data)
print(dev_data)
print(test_data)

# 형태소 단위로 토큰화된 텍스트를 저장
# train_data = train_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})
# dev_data = dev_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})
# test_data = test_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})

# print(train_data[0])

# import matplotlib.pyplot as plt # 데이터 탐색과 시각화


# plt.hist(data['train']['label'],color='red') # 훈련 셋 내 레이블 별 데이터 개수
# plt.show()
# plt.hist(data['validation']['label'],color='blue')
# plt.show()
# plt.hist(data['test']['label'],color='green')
# plt.show()

# bag-of-words 방식으로 벡터화 구현
# CountVectorizer : Bag-of-words 벡터화 구현을 위한 클래스
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from konlpy.tag import Komoran

ko_stop_words = ['은', '는', '이', '가', '께서', '에서', '부터', '까지', '에게', '한테',
'와', '과', '을', '를', '의', '로서', '로', '그리고', '즉', '게다가', '그런데', '따라서', 
'그러나', '그래도', '오히려', '비록', '또는', '및', '고', '어', '으려고', '지만', '다', '는다',
'냐', '으냐', '요', '어요', '때', '사람', '영화', '강아지', '컴퓨터', '언어' ]
vectorizer = CountVectorizer(stop_words=ko_stop_words, ngram_range=(1,2)) # 데이터 벡터화를 해주는 모델
vectorizer.fit(train_data['text']) 
# fit : 데이터를 학습시키는 메소드, 데이터의 통계량을 계산하기 위해 사용됨
# 텍스트 문서 모음을 토큰 수의 행렬로 변환

# print(vectorizer.vocabulary_) # 텍스트 문서에 나타난 어휘의 집합을 출력
print(len(vectorizer.vocabulary_)) # 텍스트 문서에 나타난 어휘 집합의 길이를 출력

train_vectors = vectorizer.transform(train_data['text']) # 학습 데이터를 숫자로 변환
dev_vectors = vectorizer.transform(dev_data['text']) # 검증 데이터를 숫자로 변환
test_vectors = vectorizer.transform(test_data['text']) # 테스트 데이터를 숫자로 변환
# transform : fit에 의해 계산된 통계량을 기준으로 입력을 변환하는 메소드, 학습시킨 것을 적용하는 과정

# print(train_vectors) # 변환된 결과 확인



# Bag-of-words 방식으로 벡터화 된 것 확인하는 방법 구현
sample_num = -1 # 확인하고자 하는 샘플 번호
sample_origin = train_data[sample_num] # 확인하고자 하는 샘플의 원문
sample_transform = train_vectors[sample_num] # 확인하고자 하는 샘플의 변환 결과
sample_inverse_transform = vectorizer.inverse_transform(sample_transform) # 변환된 결과를 다시 단어의 조합으로로

print("원래 문장 : {}\n변환 결과 : {}\n변환 결과 단어의 조합 : {}".format(sample_origin,sample_transform,sample_inverse_transform))

# SVM 분류 모델을 사용해 학습

from sklearn.svm import LinearSVC # LinearSVC : 선형 SVM을 사용하기 위한 클래스 

svm = LinearSVC() # 모델 정의
svm.fit(train_vectors, train_data['label']) # 모델 학습
print("weight : ", svm.coef_) # w값 : weight
print("bias : ", svm.intercept_) # b값 : bias

# 5-fold 교차 검증

from sklearn.model_selection import cross_val_score

all_data = train_data['text'] + dev_data['text'] + test_data['text'] # all_data 정의 
all_label = train_data['label'] + dev_data['label'] + test_data['label'] # all_label 정의 
all_vectors = vectorizer.transform(all_data) # all_data를 숫자로 변환 
scores = cross_val_score(svm, all_vectors, all_label, cv=5) # 5-fold validation 

print("전체 점수 : ", scores)
print("평균 점수 : ", format(scores.mean()*100))
print("표준 편차 : {:.6f}".format(scores.std()) )

# 그리드 탐색 : 탐색하고자 하는 하이퍼파라미터와 시도해볼만한 값을 조합해 테스트하는 방법

# from sklearn.model_selection import GridSearchCV

# param_grid = [{'max_iter':[500, 1000, 5000], "C":[1, 10, 100]}] # max_iter와 C를 변동 하이퍼파라미터로 설정
# grid_search = GridSearchCV(svm, param_grid, cv=3) # 하이퍼파라미터 서치할 모델 설정, 각 모델 별 5cross검증
# grid_search.fit(train_vectors, train_data['label']) # transformd train data로 학습 

# print(grid_search.cv_results_['mean_test_score']) # 파라미터 서치 결과 
# print(grid_search.best_params_) # 최고의 모델 파라미터 

# 파리미터 탐색 결과 출력
# for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):
#   print(mean_score, params)

# 테스트로 모델 평가

from sklearn.metrics import accuracy_score # 정확도 측정 함수 import 

# final_model = grid_search.best_estimator_ # 그리드 서치에 의해 정해진 최선의 하이퍼파라미터
final_model = svm # 기본 값으로 학습된 모델을 최종 모델로 사용

pred_results = final_model.predict(test_vectors) # 최종 모델로 test데이터 예측
accuracy = accuracy_score(test_data['label'], pred_results) # 정확도 예측 

print("정확도 : {:.2f}%".format(accuracy*100)) # 정확도 출력